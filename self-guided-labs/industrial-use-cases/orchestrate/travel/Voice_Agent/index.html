<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Speech and text - IBM watsonx Assistant web chat toolkit</title>
  <style>
    #WACContainer.WACContainer .RecordButton.cds--btn {
      width: 100%;
      max-width: unset;
      padding: 10px;
      justify-content: center;
      margin-bottom: 1px;
    }
    #WACContainer.WACContainer .RecordButton.cds--btn svg {
      width: 24px;
      height: 24px;
      margin-right: 8px;
    }
    #WACContainer.WACContainer .RecordButton.cds--btn path {
      fill: #ffffff;
    }
  </style>
</head>
<body>

  <script>
    let serviceTokens;
    let webChatInstance;
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let audioQueue = [];
    let isPlayingAudio = false;

    function generateTextFromMessage(message) {
      return message.output.generic.map(message => message.text).join(' ');
    }

    async function handleMessageReceive(event) {
  try {
    const synthText = generateTextFromMessage(event.data);
    console.log(synthText);

    const mediaSource = new MediaSource();
    const audio = new Audio(URL.createObjectURL(mediaSource));

    mediaSource.addEventListener('sourceopen', async () => {
      try {
        const sourceBuffer = mediaSource.addSourceBuffer('audio/webm;codecs=opus');
        const response = await fetch(`https://application-e0.1tgya61cxec2.us-south.codeengine.appdomain.cloud/synthesize/`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text: synthText }),
        });
        
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const reader = response.body.getReader();
        let firstChunk = true;

        // Helper function to wait for sourceBuffer to be ready
        const waitForSourceBuffer = () => {
          return new Promise((resolve) => {
            if (!sourceBuffer.updating) {
              resolve();
            } else {
              sourceBuffer.addEventListener('updateend', resolve, { once: true });
            }
          });
        };
        
        const stream = new ReadableStream({
          async start(controller) {
            async function push() {
              try {
                const { done, value } = await reader.read();
                
                if (done) {
                  console.log('Stream complete');
                  await waitForSourceBuffer();
                  mediaSource.endOfStream();
                  controller.close();
                  return;
                }

                // Wait for previous operation to complete
                await waitForSourceBuffer();
                sourceBuffer.appendBuffer(value);
                controller.enqueue(value);
                
                if (firstChunk) {
                  audio.play().catch(e => console.error('Error playing audio:', e));
                  firstChunk = false;
                }
                
                push();
              } catch (error) {
                console.error('Error in push:', error);
                controller.error(error);
              }
            }
            push();
          }
        });

        await new Response(stream).blob();
      } catch (error) {
        console.error('Error in sourceopen handler:', error);
      }
    });

    // Handle media errors
    audio.addEventListener('error', (e) => {
      console.error('Audio error:', e);
    });

  } catch (error) {
    console.error('Error in handleMessageReceive:', error);
  }
}

    function sendTextToAssistant(text) {
      const sendObject = { input: { text } };
      const sendOptions = {
        silent: false 
      };
      webChatInstance.send(sendObject, sendOptions);
    }

    function onStartRecord() {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          mediaRecorder = new MediaRecorder(stream);
          mediaRecorder.start();

          mediaRecorder.ondataavailable = event => {
            audioChunks.push(event.data);
          };

          mediaRecorder.onstop = () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
    const formData = new FormData();
    formData.append('file', audioBlob, 'recording.wav');  // Changed from 'audio' to 'file'

    fetch('https://application-e0.1tgya61cxec2.us-south.codeengine.appdomain.cloud/process-audio/', {
        method: 'POST',
        body: formData,
    })
    .then(response => {
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return response.json();
    })
    .then(data => {
        if (data.text) {
            sendTextToAssistant(data.text);
        } else {
            console.error('No text in response');
        }
    })
    .catch(error => {
        console.error('Error:', error);
        // You might want to show an error message to the user here
    });

    audioChunks = [];  // Clear the chunks for next recording
};
        })
        .catch(error => {
          console.error('Error accessing microphone:', error);
        });
    }

    function onStopRecord() {
      if (mediaRecorder) {
        mediaRecorder.stop();
      }
    }

    function setButtonState(localIsRecording) {
      isRecording = localIsRecording;
      document.querySelector('.RecordButton').innerHTML = getButtonHTML(isRecording);
    }

    function getButtonHTML(isRecording) {
      let buttonContent;
      if (isRecording) {
        buttonContent = `
          <svg viewBox="0 0 32 32">
            <path d="M23,14v3A7,7,0,0,1,9,17V14H7v3a9,9,0,0,0,8,8.94V28H11v2H21V28H17V25.94A9,9,0,0,0,25,17V14Z" />
            <path d="M16,22a5,5,0,0,0,5-5V7A5,5,0,0,0,11,7V17A5,5,0,0,0,16,22Z" />
          </svg>
          Stop recording
        `;
      } else {
        buttonContent = `
          <svg viewBox="0 0 32 32">
            <path d="M23,14v3A7,7,0,0,1,9,17V14H7v3a9,9,0,0,0,8,8.94V28H11v2H21V28H17V25.94A9,9,0,0,0,25,17V14Z"/>
            <path d="M16,22a5,5,0,0,0,5-5V7A5,5,0,0,0,11,7V17A5,5,0,0,0,16,22ZM13,7a3,3,0,0,1,6,0V17a3,3,0,0,1-6,0Z"/>
          </svg>
          Start recording
        `;
      }
      return buttonContent;
    }

    function onButtonClick() {
      const newRecordingState = !isRecording;
      setButtonState(newRecordingState);
      if (newRecordingState) {
        onStartRecord();
      } else {
        onStopRecord();
      }
    }

    function addRecordButton(instance) {
      const button = document.createElement('button');
      button.classList.add('RecordButton');
      button.classList.add('cds--btn');
      button.classList.add('cds--btn--primary');
      button.innerHTML = getButtonHTML(false);
      button.onclick = onButtonClick;
      instance.writeableElements.beforeInputElement.appendChild(button);
    }

    async function onLoad(instance) {
      webChatInstance = instance;

      instance.on({ type: 'receive', handler: handleMessageReceive });
      addRecordButton(instance);
      instance.updateHomeScreenConfig({ is_on: false });
      await instance.render();
    }

    window.watsonAssistantChatOptions = {
    integrationID: "a16c3364-5bee-4975-8b2c-84a654d02a59", // The ID of this integration.
    region: "wxo-us-south", // The region your integration is hosted in.
    serviceInstanceID: "2d3baa3f-8b29-4871-bb68-4b6bef054ce2", // The ID of your service instance.
    onLoad: onLoad
  };
  setTimeout(function(){
    const t=document.createElement('script');
    t.src="https://web-chat.global.assistant.watson.appdomain.cloud/versions/" + (window.watsonAssistantChatOptions.clientVersion || 'latest') + "/WatsonAssistantChatEntry.js";
    document.head.appendChild(t);
  });
  </script>

</body>
</html>
